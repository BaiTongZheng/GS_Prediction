{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "import logging\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.stats import stats\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "import os\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import ast\n",
    "import pandas.io.json as pdjson\n",
    "warnings.simplefilter('error', SettingWithCopyWarning)\n",
    "gc.enable()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/baitong/pywork/RevenuePrediction/all (1)/train_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "def load_df(csv_path,n):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    ans = pd.DataFrame()\n",
    "    dfs = pd.read_csv(csv_path, sep=',',\n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                    chunksize = 100000)\n",
    "    count = 0\n",
    "    for df in dfs:\n",
    "        count+=1\n",
    "        if count==n:\n",
    "            df.reset_index(drop = True,inplace = True)       \n",
    "            for column in JSON_COLUMNS:\n",
    "                column_as_df = json_normalize(df[column])\n",
    "                column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "                df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "\n",
    "            print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "            use_df = df\n",
    "            del df\n",
    "            gc.collect()\n",
    "            ans = pd.concat([ans, use_df], axis = 0).reset_index(drop = True)\n",
    "            print(ans.shape)\n",
    "            break\n",
    "        else :\n",
    "            continue\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(df):\n",
    "    df['hits']=df['hits'].apply(ast.literal_eval)\n",
    "    df['hits']=df['hits'].str[0]\n",
    "    df['hits']=df['hits'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n",
    "    \n",
    "    df['customDimensions']=df['customDimensions'].apply(ast.literal_eval)\n",
    "    df['customDimensions']=df['customDimensions'].str[0]\n",
    "    df['customDimensions']=df['customDimensions'].apply(lambda x: {'index':np.NaN,'value':np.NaN} if pd.isnull(x) else x)\n",
    "    \n",
    "    JSON_COLUMNS = ['hits','customDimensions']\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = pdjson.json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "        print(\"'parse' function to flatten JSON columns :\",column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "useless_feature1 = ['trafficSource_adwordsClickInfo.gclId','hits_contentGroup.previousContentGroup1','hits_contentGroup.previousContentGroup2',\n",
    "'hits_contentGroup.previousContentGroup3','hits_contentGroup.previousContentGroup4',\n",
    "'hits_contentGroup.previousContentGroup5','hits_customDimensions',\n",
    "'hits_customMetrics','hits_customVariables',            \n",
    "                  ]\n",
    "# ['hits_latencyTracking.domainLookupTime'\\n 'hits_latencyTracking.redirectionTime'\\n 'hits_latencyTracking.serverConnectionTime']\n",
    "useless_feature2 = ['hits_latencyTracking.domContentLoadedTime',\n",
    "'hits_latencyTracking.domInteractiveTime',\n",
    "'hits_latencyTracking.domLatencyMetricsSample',\n",
    "'hits_latencyTracking.pageDownloadTime',\n",
    "'hits_latencyTracking.pageLoadSample',\n",
    "'hits_latencyTracking.pageLoadTime',\n",
    "'hits_latencyTracking.serverResponseTime',\n",
    "'hits_latencyTracking.speedMetricsSample']\n",
    "useless_feature3 = ['hits_publisher_infos','hits_contentGroup.contentGroup1',\n",
    "'hits_contentGroup.contentGroup2',\n",
    "'hits_contentGroup.contentGroup3',\n",
    "'hits_contentGroup.contentGroup4',\n",
    "'hits_contentGroup.contentGroup5',\n",
    "'hits_contentGroup.contentGroupUniqueViews1',\n",
    "'hits_contentGroup.contentGroupUniqueViews2',\n",
    "'hits_contentGroup.contentGroupUniqueViews3']\n",
    "useless_feature4 = ['hits_eventInfo.eventAction','hits_eventInfo.eventCategory','hits_eventInfo.eventLabel',\n",
    "                   'hits_experiment'\n",
    "                  ]\n",
    "useless_feature = useless_feature1+useless_feature2+useless_feature3+useless_feature4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelect(df):\n",
    "    #获取不变的常量列，模型无法在常量数据计学到东西，数据与处理时需要drop\n",
    "    const_cols = [c for c in df.columns if df[c].nunique(dropna=False)==1 ]\n",
    "#     print(const_cols)    \n",
    "    df = df.drop(const_cols , axis=1)\n",
    "    df = parseData(df)\n",
    "    df = df.drop(labels=['hits_product'], axis=1)\n",
    "    df = df.drop(labels=['hits_promotion'], axis=1)\n",
    "    df = df.drop(labels=useless_feature, axis=1)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chunksize 6 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 6 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 7 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 7 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 8 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 8 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 9 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 9 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 10 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 10 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 11 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 11 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 12 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 12 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 13 / 18 :\n",
      "Loaded train_v2.csv. Shape: (100000, 59)\n",
      "(100000, 59)\n",
      "'parse' function to flatten JSON columns : hits\n",
      "'parse' function to flatten JSON columns : customDimensions\n",
      "loaded chunksize 13 / 18 :\n",
      "--------------------------------------------------\n",
      "loading chunksize 14 / 18 :\n"
     ]
    }
   ],
   "source": [
    "for i in range(18):\n",
    "    if i >= 5:\n",
    "        print(\"loading chunksize %d / 18 :\"%(i+1))\n",
    "        train = load_df(path,i+1)\n",
    "        train = featureSelect(train)\n",
    "        train.to_csv(\"/home/baitong/pywork/RevenuePrediction/train0\" +str(i) +'.csv',index = False)\n",
    "        print(\"loaded chunksize %d / 18 :\"%(i+1))\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonlist=[]\n",
    "# for i in range(len(train.columns)):   # for each column\n",
    "#     if (isinstance(train.iloc[1,i], list) ):  # see if some element 1 is a list\n",
    "#         jsonlist.append( train.columns[i] )   # if yes, then save name to list\n",
    "# print(jsonlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Printout for each column's number of unique values (incl. nans)\\n\")\n",
    "# for col in train.columns:\n",
    "#     try:\n",
    "#         print(col, ':', train[col].nunique(dropna=False))\n",
    "#     except TypeError:\n",
    "#         a=train[col].astype('str')\n",
    "#         #print(a)\n",
    "#         print( col, ':', a.nunique(dropna=False), ' >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> LIST')\n",
    "# # Clean workspace\n",
    "# del(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Data shape before dropping constant columns:', train.shape)\n",
    "\n",
    "# print('\\nColumns being dropped:')\n",
    "\n",
    "# for col in train.columns:\n",
    "#     try:\n",
    "#         if (train[col].nunique(dropna=False) == 1):\n",
    "#             del(train[col])\n",
    "#             print(col)\n",
    "#     except TypeError:\n",
    "#         a=train[col].astype('str')\n",
    "#         if (a.nunique(dropna=False) == 1):\n",
    "#             del(train[col])\n",
    "#             print(col)\n",
    "# del(col)\n",
    "\n",
    "# print('\\ndata shape is now:', train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printout for each column's number of unique values (incl. nans)\\n\")\n",
    "for col in train.columns:\n",
    "    try:\n",
    "        print(col)\n",
    "    except TypeError:\n",
    "        a=train[col].astype('str')\n",
    "        #print(a)\n",
    "        print( col)\n",
    "# Clean workspace\n",
    "del(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
