{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import gc\n",
    "import time\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import os\n",
    "warnings.simplefilter('error', SettingWithCopyWarning)\n",
    "gc.enable()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path='D:/zhiliao/kaggle/predict_income/train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "#     print(df.head())\n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/baitong/pywork/RevenuePrediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Shape: (903653, 55)\n",
      "Loaded test.csv. Shape: (804684, 53)\n",
      "Wall time: 6min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = load_df()\n",
    "test = load_df(csv_path='D:/zhiliao/kaggle/predict_income/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channelGrouping                object\n",
    "# date                           object\n",
    "# fullVisitorId                  object\n",
    "# sessionId                      object\n",
    "# visitId                        object\n",
    "# visitNumber                   float64\n",
    "# visitStartTime                float64\n",
    "# device.browser                 object\n",
    "# device.deviceCategory          object\n",
    "# device.isMobile               float64\n",
    "# device.operatingSystem         object\n",
    "# geoNetwork.city                object\n",
    "# geoNetwork.continent           object\n",
    "# geoNetwork.country             object\n",
    "# geoNetwork.metro               object\n",
    "# geoNetwork.networkDomain       object\n",
    "# geoNetwork.region              object\n",
    "# geoNetwork.subContinent        object\n",
    "# totals.bounces                float64\n",
    "# totals.hits                   float64\n",
    "# totals.newVisits              float64\n",
    "# totals.pageviews              float64\n",
    "# totals.transactionRevenue     float64\n",
    "# trafficSource.adContent        object\n",
    "# trafficSource.campaign         object\n",
    "# trafficSource.isTrueDirect    float64\n",
    "# trafficSource.keyword          object\n",
    "# trafficSource.medium           object\n",
    "# trafficSource.referralPath     object\n",
    "# trafficSource.source           object\n",
    "# dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store_1 = pd.read_csv('D:/zhiliao/kaggle/predict_income/external_data/Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "train_store_2 = pd.read_csv('D:/zhiliao/kaggle/predict_income/external_data/Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_1 = pd.read_csv('D:/zhiliao/kaggle/predict_income/external_data/Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_2 = pd.read_csv('D:/zhiliao/kaggle/predict_income/external_data/Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n",
    "    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_exdata = pd.concat([train_store_1, train_store_2], sort=False)\n",
    "test_exdata = pd.concat([test_store_1, test_store_2], sort=False)\n",
    "\n",
    "for df in [train, test]:\n",
    "    df[\"visitId\"] = df[\"visitId\"].astype(str)\n",
    "\n",
    "# Merge with train/test data\n",
    "train_new = train.merge(train_exdata, how=\"left\", on=\"visitId\")\n",
    "test_new = test.merge(test_exdata, how=\"left\", on=\"visitId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Client Id\n",
    "for df in [train_new, test_new]:\n",
    "    df.drop(\"Client Id\", 1, inplace=True)\n",
    "\n",
    "#Cleaning Revenue\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Revenue\"].fillna('$', inplace=True)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n",
    "    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n",
    "    df[\"Revenue\"].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_new, test_new]:\n",
    "    df[\"Sessions\"] = df[\"Sessions\"].fillna(0)\n",
    "    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].fillna(0)\n",
    "    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].fillna(0)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].fillna(0)\n",
    "    df[\"Transactions\"] = df[\"Transactions\"].fillna(0)\n",
    "    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].fillna(0)\n",
    "    df['trafficSource_adContent'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource_isTrueDirect'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource_referralPath'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource_keyword'].fillna('N/A', inplace=True)\n",
    "    df['totals_bounces'].fillna(0.0, inplace=True)\n",
    "    df['totals_newVisits'].fillna(0.0, inplace=True)\n",
    "    df['totals_pageviews'].fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new[train_new.Sessions>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "del test\n",
    "train = train_new\n",
    "test = test_new\n",
    "del train_new\n",
    "del test_new\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取不变的常量列，模型无法在常量数据计学到东西，数据与处理时需要drop\n",
    "const_cols = [c for c in train.columns if train[c].nunique(dropna=False)==1 ]\n",
    "print(const_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_cols2=['socialEngagementType', 'device_browserSize', 'device_browserVersion', 'device_flashVersion', 'device_language', 'device_mobileDeviceBranding', 'device_mobileDeviceInfo', 'device_mobileDeviceMarketingName', 'device_mobileDeviceModel', 'device_mobileInputSelector', 'device_operatingSystemVersion', 'device_screenColors', 'device_screenResolution', 'geoNetwork_cityId', 'geoNetwork_latitude', 'geoNetwork_longitude', 'geoNetwork_networkLocation', 'totals_visits', 'trafficSource_adwordsClickInfo.criteriaParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(const_cols , axis=1)\n",
    "test = test.drop(const_cols2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####删除test中不存在的列\n",
    "train = train.drop([\"trafficSource_campaignCode\"], axis=1)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['totals_transactionRevenue'] = train['totals_transactionRevenue'].fillna(0).astype(\"float\")\n",
    "# del train['totals_transactionRevenue']\n",
    "\n",
    "# if 'totals_transactionRevenue' in test.columns:\n",
    "#     del test['totals_transactionRevenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['trafficSource_adwordsClickInfo.adNetworkType','trafficSource_adwordsClickInfo.gclId',\n",
    "          'trafficSource_adwordsClickInfo.isVideoAd','trafficSource_adwordsClickInfo.page',\n",
    "           'trafficSource_adwordsClickInfo.slot'],axis =1,inplace = True)\n",
    "test.drop(['trafficSource_adwordsClickInfo.adNetworkType','trafficSource_adwordsClickInfo.gclId',\n",
    "          'trafficSource_adwordsClickInfo.isVideoAd','trafficSource_adwordsClickInfo.page',\n",
    "        'trafficSource_adwordsClickInfo.slot'],axis =1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.visitStartTime = pd.to_datetime(train.visitStartTime, unit='s')\n",
    "test.visitStartTime = pd.to_datetime(test.visitStartTime, unit='s')\n",
    "train[\"date\"] = train.visitStartTime\n",
    "test[\"date\"] = test.visitStartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearRare(columnname, limit = 1000):\n",
    "    # you may search for rare categories in train, train&test, or just test\n",
    "    #vc = pd.concat([train[columnname], test[columnname]], sort=False).value_counts()\n",
    "    vc = test[columnname].value_counts()\n",
    "    \n",
    "    common = vc > limit\n",
    "    common = set(common.index[common].values)\n",
    "    print(\"Set\", sum(vc <= limit), columnname, \"categories to 'other';\", end=\" \")\n",
    "    \n",
    "    train.loc[train[columnname].map(lambda x: x not in common), columnname] = 'other'\n",
    "    test.loc[test[columnname].map(lambda x: x not in common), columnname] = 'other'\n",
    "    print(\"now there are\", train[columnname].nunique(), \"categories in train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dealMissingData(df):\n",
    "    for c in df.columns:\n",
    "        num_missing = df[c].isnull().sum() \n",
    "        if num_missing != 0:\n",
    "            print(\"missing col is  \",c,'   missing num is',num_missing)\n",
    "    num_feature = [\"totals_bounces\",\"totals_newVisits\"]\n",
    "    cat_feature = ['trafficSource_adContent','trafficSource_adContent',\n",
    "                  'trafficSource_campaign','trafficSource_isTrueDirect',\n",
    "                  'trafficSource_keyword','trafficSource_referralPath']\n",
    "    for col in num_feature:\n",
    "        df[col].fillna(0,inplace=True)\n",
    "    for col in cat_feature:\n",
    "        df[col].fillna(-999,inplace=True)\n",
    "    for c in df.columns:\n",
    "        num_missing = df[c].isnull().sum() \n",
    "        if num_missing != 0:\n",
    "            print(\"missing col is  \",c,'   missing num is',num_missing)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dealMissingData(train)\n",
    "test = dealMissingData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearRare(\"device_browser\")\n",
    "clearRare(\"device_operatingSystem\")\n",
    "clearRare(\"geoNetwork_country\")\n",
    "clearRare(\"geoNetwork_city\")\n",
    "clearRare(\"geoNetwork_metro\")\n",
    "clearRare(\"geoNetwork_networkDomain\")\n",
    "clearRare(\"geoNetwork_region\")\n",
    "clearRare(\"geoNetwork_subContinent\")\n",
    "clearRare(\"trafficSource_adContent\")\n",
    "clearRare(\"trafficSource_campaign\")\n",
    "clearRare(\"trafficSource_keyword\")\n",
    "clearRare(\"trafficSource_medium\")\n",
    "clearRare(\"trafficSource_referralPath\")\n",
    "clearRare(\"trafficSource_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index(\"visitStartTime\", inplace=True)\n",
    "test.set_index(\"visitStartTime\", inplace=True)\n",
    "train.sort_index(inplace=True)\n",
    "test.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###填补totals.pageviews值\n",
    "predmis = train[train['totals_pageviews'].isnull()]['totals_hits'].values\n",
    "train.loc[(train['totals_pageviews'].isnull()),'totals_pageviews' ] = predmis\n",
    "\n",
    "predmis = test[test['totals_pageviews'].isnull()]['totals_hits'].values\n",
    "test.loc[(test['totals_pageviews'].isnull()),'totals_pageviews' ] = predmis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['weekday'] = df['date'].dt.dayofweek.astype(object)\n",
    "    df['time'] = df['date'].dt.second + df['date'].dt.minute*60 + df['date'].dt.hour*3600\n",
    "    #df['month'] = df['date'].dt.month   # it must not be included in features during learning!\n",
    "    df['day'] = df['date'].dt.date       # it must not be included in features during learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['weekday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test])\n",
    "df.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\n",
    "df['prev_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)).astype(np.int64) // 1e9 // 60 // 60\n",
    "df['next_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)).astype(np.int64) // 1e9 // 60 // 60\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "train = df[:len(train)]\n",
    "test = df[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['source_country'] = df['trafficSource_source'] + '_' + df['geoNetwork_country']\n",
    "    df['campaign_medium'] = df['trafficSource_campaign'] + '_' + df['trafficSource_medium']\n",
    "    df['browser_category'] = df['device_browser'] + '_' + df['device_deviceCategory']\n",
    "    df['browser_os'] = df['device_browser'] + '_' + df['device_operatingSystem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['device_deviceCategory_channelGrouping'] = df['device_deviceCategory'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_browser'] = df['device_browser'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_OS'] = df['device_operatingSystem'] + \"_\" + df['channelGrouping']\n",
    "    \n",
    "    for i in ['geoNetwork_city', 'geoNetwork_continent', 'geoNetwork_country','geoNetwork_metro', 'geoNetwork_networkDomain', 'geoNetwork_region','geoNetwork_subContinent']:\n",
    "        for j in ['device_browser','device_deviceCategory', 'device_operatingSystem', 'trafficSource_source']:\n",
    "            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n",
    "    \n",
    "    df['content_source'] = df['trafficSource_adContent'].astype(str) + \"_\" + df['source_country']\n",
    "    df['medium_source'] = df['trafficSource_medium'] + \"_\" + df['source_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['totals_hits'] = train['totals_hits'].astype(\"float\")\n",
    "test['totals_hits'] = test['totals_hits'].astype(\"float\")\n",
    "train['totals_pageviews'] = train['totals_pageviews'].astype(\"float\")\n",
    "test['totals_pageviews'] = test['totals_pageviews'].astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in [\"totals_hits\", \"totals_pageviews\"]:\n",
    "    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].mean()\n",
    "    train[\"usermean_\" + feature] = train.fullVisitorId.map(info)\n",
    "    test[\"usermean_\" + feature] = test.fullVisitorId.map(info)\n",
    "    \n",
    "for feature in [\"visitNumber\"]:\n",
    "    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].max()\n",
    "    train[\"usermax_\" + feature] = train.fullVisitorId.map(info)\n",
    "    test[\"usermax_\" + feature] = test.fullVisitorId.map(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['date', 'fullVisitorId', 'sessionId', 'totals_transactionRevenue', 'visitId', 'visitStartTime', \n",
    "            'month', 'day', 'help']\n",
    "\n",
    "cat_cols = [f for f in train.columns if (train[f].dtype == 'object' and f not in excluded)]\n",
    "real_cols = [f for f in train.columns if (not f in cat_cols and f not in excluded)]\n",
    "print(\"cat_cols: \",cat_cols)\n",
    "print(\"real_cols: \",real_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[cat_cols].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in cat_cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n",
    "    train[col] = lbl.transform(list(train[col].values.astype('str')))\n",
    "    test[col] = lbl.transform(list(test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in real_cols:\n",
    "    train[col] = train[col].astype(float)\n",
    "    test[col] = test[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = train['totals_transactionRevenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = get_folds(df=train, n_splits=5)\n",
    "\n",
    "train_features = real_cols + cat_cols\n",
    "importances = pd.DataFrame()\n",
    "oof_reg_preds = np.zeros(train.shape[0])\n",
    "sub_reg_preds = np.zeros(test.shape[0])\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n",
    "    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x,np.log1p(trn_y),\n",
    "        eval_set=[(val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = train_features\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_reg_preds[oof_reg_preds < 0] = 0\n",
    "    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_reg_preds += np.expm1(_preds) / len(folds)\n",
    "    \n",
    "mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oof_reg_preds),len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sub_reg_preds),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predictions'] = np.expm1(oof_reg_preds)\n",
    "test['predictions'] = sub_reg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算单个用户会话总数\n",
    "train[\"sess_count\"]=1;\n",
    "train_sess_num = train[['fullVisitorId', 'sess_count']].groupby('fullVisitorId').sum()\n",
    "test[\"sess_count\"]=1;\n",
    "test_sess_num = test[['fullVisitorId', 'sess_count']].groupby('fullVisitorId').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###将大于20次的会话数量限制为20 （大于20次数据过少）\n",
    "train_sess_num['sess_count']=train_sess_num['sess_count'].apply(lambda x: 20 if x>20 else x)\n",
    "test_sess_num['sess_count']=test_sess_num['sess_count'].apply(lambda x: 20 if x>20 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sess_num['sess_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####这里采用均值合并，对于类型特征来说并不合理\n",
    "train_agg = train[train_features+['fullVisitorId']].groupby('fullVisitorId').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId').sum()\n",
    "len(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fulldata = pd.concat([train_agg, train_pred,train_sess_num], axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agg = test[train_features+['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "test_pred = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId').sum()\n",
    "test_fulldata = pd.concat([test_agg, test_pred,test_sess_num], axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fulldata.shape,train_fulldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = y_reg\n",
    "trn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fulldata.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = get_folds(df=train_fulldata[['totals_pageviews']].reset_index(), n_splits=5)\n",
    "\n",
    "lgb_oof_preds = np.zeros(train_fulldata.shape[0])\n",
    "lgb_sub_preds = np.zeros(test_fulldata.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train_fulldata.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = train_fulldata.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        eval_names=['TRAIN', 'VALID'],\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = trn_x.columns\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    lgb_oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    lgb_oof_preds[lgb_oof_preds < 0] = 0\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = reg.predict(test_fulldata[train_fulldata.columns], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    lgb_sub_preds += _preds / len(folds)\n",
    "    \n",
    "mean_squared_error(np.log1p(trn_user_target['target']), lgb_oof_preds) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\n",
    "mean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\n",
    "vis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "plt.figure(figsize=(8, 25))\n",
    "sns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fulldata['PredictedLogRevenue'] = lgb_sub_preds\n",
    "test_fulldata[['PredictedLogRevenue']].to_csv('/home/baitong/pywork/RevenuePrediction/submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "        'objective': 'reg:linear',\n",
    "        'booster': 'gbtree',\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 22,\n",
    "        'min_child_weight': 57,\n",
    "        'gamma' : 1.45,\n",
    "        'alpha': 0.0,\n",
    "        'lambda': 0.0,\n",
    "        'subsample': 0.67,\n",
    "        'colsample_bytree': 0.054,\n",
    "        'colsample_bylevel': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 456\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "fit_params={\"early_stopping_rounds\": 50, \"verbose\": 100, \"eval_metric\": \"rmse\"}\n",
    "\n",
    "folds = get_folds(df=train_fulldata[['totals_pageviews']].reset_index(), n_splits=5)\n",
    "\n",
    "xgb_oof_preds = np.zeros(train_fulldata.shape[0])\n",
    "xgb_sub_preds = np.zeros(test_fulldata.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train_fulldata.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = train_fulldata.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    xgb = XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "    \n",
    "    xgb.fit(trn_x, np.log1p(trn_y),\n",
    "            eval_set=[(val_x, np.log1p(val_y))],\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric='rmse',\n",
    "            verbose=100)\n",
    "    \n",
    "    xgb_oof_preds[val_] = xgb.predict(val_x)\n",
    "    xgb_oof_preds[xgb_oof_preds < 0] = 0\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = xgb.predict(test_fulldata[train_fulldata.columns])\n",
    "    _preds[_preds < 0] = 0\n",
    "    xgb_sub_preds += _preds / len(folds)\n",
    "    \n",
    "mean_squared_error(np.log1p(trn_user_target['target']), xgb_oof_preds) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_param = {\n",
    "    'learning_rate' :0.03,\n",
    "    'depth' :10,\n",
    "    'eval_metric' :'RMSE',\n",
    "    'od_type' :'Iter',\n",
    "    'metric_period ' : 50,\n",
    "    'od_wait' : 20,\n",
    "    'seed' : 42\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "folds = get_folds(df=train_fulldata[['totals_pageviews']].reset_index(), n_splits=5)\n",
    "\n",
    "cat_oof_preds = np.zeros(train_fulldata.shape[0])\n",
    "cat_sub_preds = np.zeros(test_fulldata.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train_fulldata.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = train_fulldata.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    cat = CatBoostRegressor(iterations=1000,learning_rate=0.03,\n",
    "                            depth=10,\n",
    "                            eval_metric='RMSE',\n",
    "                            random_seed = 42,\n",
    "                            bagging_temperature = 0.2,\n",
    "                            od_type='Iter',\n",
    "                            metric_period = 50,\n",
    "                            od_wait=20)\n",
    "    cat.fit(trn_x, np.log1p(trn_y), \n",
    "            eval_set=[(val_x, np.log1p(val_y))],\n",
    "            early_stopping_rounds=50,\n",
    "            use_best_model=True,\n",
    "            verbose=100)\n",
    "    cat_oof_preds[val_] = cat.predict(val_x)\n",
    "    cat_oof_preds[cat_oof_preds < 0] = 0\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = cat.predict(test_fulldata[train_fulldata.columns])\n",
    "    _preds[_preds < 0] = 0\n",
    "    cat_sub_preds += _preds / len(folds)\n",
    "    \n",
    "mean_squared_error(np.log1p(trn_user_target['target']), cat_oof_preds) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fulldata['lgb_pred'] = lgb_sub_preds\n",
    "test_fulldata['xgb_pred'] = xgb_sub_preds\n",
    "test_fulldata['cst_pred'] = cat_sub_preds\n",
    "test_fulldata[['lgb_pred','xgb_pred','cst_pred']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
